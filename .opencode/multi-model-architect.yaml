# Multi-Model Architect Configuration
# Run architect agent with multiple models to compare spec generation

models:
  # GitHub Copilot Models
  - name: "gpt-4o"
    provider: "github-copilot"
    display_name: "GPT-4o (GitHub Copilot)"
    enabled: true
    
  - name: "claude-3.5-sonnet"
    provider: "github-copilot"
    display_name: "Claude 3.5 Sonnet (GitHub Copilot)"
    enabled: true
    
  - name: "gemini-2.0-flash-exp"
    provider: "github-copilot"
    display_name: "Gemini 2.0 Flash (GitHub Copilot)"
    enabled: true
    
  - name: "o1-preview"
    provider: "github-copilot"
    display_name: "O1 Preview (GitHub Copilot)"
    enabled: true

  # Note: GPT-5.1-Codex-Max doesn't exist yet - using o1 instead
  # Note: Gemini 3 Pro doesn't exist yet - using Gemini 2.0 Flash instead
  
  # OpenCode Zen Models (if available)
  # - name: "big-pickle"
  #   provider: "opencode-zen"
  #   display_name: "Big Pickle (OpenCode Zen)"
  #   enabled: false

output:
  # Where to save specs generated by each model
  directory: "specs/multi-model-comparison"
  naming_pattern: "day-{day}-{model_slug}.md"
  
agent:
  role: "architect"
  prompt_file: ".opencode/prompts/architect.txt"
  
  # Task configuration
  task:
    timeout_seconds: 300
    max_tokens: 8000
    temperature: 0.2
    
comparison:
  # Automatically generate comparison report
  generate_report: true
  report_file: "specs/multi-model-comparison/comparison-day-{day}.md"
  
  # Metrics to compare
  metrics:
    - completeness  # Did it cover all sections?
    - detail_level  # How detailed were the explanations?
    - test_coverage # How many test cases provided?
    - algorithm_analysis # Quality of algorithm recommendations
    - clarity  # How clear and actionable is the spec?
